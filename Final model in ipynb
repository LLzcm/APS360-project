{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5OaH7rNT8V4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import os\n",
        "import torch.nn as nn\n",
        "drive.mount('/content/gdrive')\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ysSVJAEd1mM"
      },
      "outputs": [],
      "source": [
        "\n",
        "path = \"/content/gdrive/MyDrive/APS360/360Project/final_dataset\"\n",
        "train_dir = os.path.join(path, 'train/')\n",
        "val_dir = os.path.join(path, 'val/')\n",
        "test_dir = os.path.join(path, 'test/')\n",
        "classes =['Autos & Vehicles','Food & Drink','Pets & Animals','Science & Education','Sports']\n",
        "\n",
        "data_transform = transforms.Compose([transforms.RandomResizedCrop(224),transforms.RandAugment(),transforms.ToTensor()])\n",
        "train_set = datasets.ImageFolder(train_dir, transform=data_transform)\n",
        "\n",
        "data_transform = transforms.Compose([transforms.RandomResizedCrop(224),transforms.ToTensor()])\n",
        "validation_set = datasets.ImageFolder(val_dir, transform=data_transform)\n",
        "test_set = datasets.ImageFolder(test_dir, transform=data_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lh99jyVYB7e"
      },
      "outputs": [],
      "source": [
        "# define dataloader parameters\n",
        "batch_size  = 500\n",
        "num_workers = 0\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size,num_workers=num_workers, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aaf6r1gRV_o2"
      },
      "outputs": [],
      "source": [
        "#Model CNN \n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class LargeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LargeNet, self).__init__()\n",
        "        self.name = \"large\"\n",
        "        self.conv1 = nn.Conv2d(3, 5, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(5, 10, 5)\n",
        "        self.conv3 = nn.Conv2d(10, 20, 4)\n",
        "        self.conv4 = nn.Conv2d(20, 40, 4)\n",
        "        self.fc1 = nn.Linear(4840, 320)\n",
        "        self.fc2 = nn.Linear(320, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = x.view(-1, 4840)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x) \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_a27FRfp5sc"
      },
      "outputs": [],
      "source": [
        "#GoogleENET\n",
        "googleNet = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoN5bvmuqVNc"
      },
      "outputs": [],
      "source": [
        "#Artifical Neural Network Architecture (GoogleNet)\n",
        "Google = googleNet\n",
        "class ANNClassifier_GOOGLE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ANNClassifier_GOOGLE, self).__init__()\n",
        "        self.fc1 = nn.Sequential(nn.Dropout(0.5),nn.Linear(1000,6))\n",
        "        #self.fc2 = nn.Linear(10, 6)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1000) #flatten feature data\n",
        "        x = F.relu(self.fc1(x))\n",
        "        #x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X801oYTZhXi"
      },
      "source": [
        "Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOm1VXF-ZfKP"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, train=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      if train:\n",
        "          data_loader = train_loader\n",
        "      else:\n",
        "          data_loader = val_loader\n",
        "\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for imgs, labels in data_loader:\n",
        "        \n",
        "         \n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          imgs = imgs.cuda()\n",
        "          labels = labels.cuda()\n",
        "        #############################################\n",
        "        #改这里 change here to switch model\n",
        "        output = model(Google(imgs))\n",
        "        \n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXdCkWkda9gk"
      },
      "source": [
        "Train code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm_XPXy6ZkdJ"
      },
      "outputs": [],
      "source": [
        "def train(model, data, lr,batch_size, num_epochs=1,train_loader=train_loader):\n",
        "    #train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
        "    # train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "                                           #num_workers=num_workers, shuffle=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "    iters, losses, train_acc, val_acc = [], [], [], []\n",
        "\n",
        "    # training\n",
        "    n = 0 # the number of iterations\n",
        "    start_time=time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        mini_b=0\n",
        "        mini_batch_correct = 0\n",
        "        Mini_batch_total = 0\n",
        "        for imgs, labels in iter(train_loader):\n",
        "          \n",
        "            \n",
        "            #############################################\n",
        "            #To Enable GPU Usage\n",
        "            if use_cuda and torch.cuda.is_available():\n",
        "              imgs = imgs.cuda()\n",
        "              labels = labels.cuda()\n",
        "            #############################################\n",
        "\n",
        "          #### ALNC is alexNet.features (AlexNet without classifier) ####\n",
        "          #### VGG is vggNet.fearures (VggNet without classifier) ####\n",
        "          #### RES is resNet\n",
        "          #改这里 change here to switch model\n",
        "            out = model(Google(imgs))             # forward pass\n",
        "            loss = criterion(out, labels) # compute the total loss\n",
        "            loss.backward()               # backward pass (compute parameter updates)\n",
        "            optimizer.step()              # make the updates for each parameter\n",
        "            optimizer.zero_grad()         # a clean up step for PyTorch\n",
        "\n",
        "\n",
        "\n",
        "            ##### Mini_batch Accuracy ##### We don't compute accuracy on the whole training set in every iteration!\n",
        "            pred = out.max(1, keepdim=True)[1]\n",
        "            mini_batch_correct = pred.eq(labels.view_as(pred)).sum().item()\n",
        "            Mini_batch_total = imgs.shape[0]\n",
        "            train_acc.append((mini_batch_correct / Mini_batch_total))\n",
        "           ###########################\n",
        "\n",
        "          # save the current training information\n",
        "            iters.append(n)\n",
        "            losses.append(float(loss)/batch_size)             # compute *average* loss\n",
        "            val_acc.append(get_accuracy(model, train=False))  # compute validation accuracy\n",
        "            n += 1\n",
        "            mini_b += 1\n",
        "            print(\"Iteration: \",n,'Progress: % 6.2f ' % ((epoch * len(train_loader) + mini_b) / (num_epochs * len(train_loader))*100),'%', \"Time Elapsed: % 6.2f s \" % (time.time()-start_time))\n",
        "\n",
        "\n",
        "        print (\"Epoch %d Finished. \" % epoch ,\"Time per Epoch: % 6.2f s \"% ((time.time()-start_time) / (epoch +1)))\n",
        "        model_path = get_model_name(\"google_drop_out_full_last\", batch_size=batch_size, learning_rate=lr, epoch=epoch)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    end_time= time.time()\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, losses, label=\"Train\")\n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(iters, train_acc, label=\"Training\")\n",
        "    plt.plot(iters, val_acc, label=\"Validation\")    \n",
        "    plt.xlabel(\"Iterations\")\n",
        "    plt.ylabel(\"Validation Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    train_acc.append(get_accuracy(model, train=True))\n",
        "    print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
        "    print(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
        "    print (\"Total time:  % 6.2f s  Time per Epoch: % 6.2f s \" % ( (end_time-start_time), ((end_time-start_time) / num_epochs) ))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3QzXDOgDYcz"
      },
      "outputs": [],
      "source": [
        "def get_model_name(name, batch_size, learning_rate, epoch):\n",
        "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
        "\n",
        "    Args:\n",
        "        config: Configuration object containing the hyperparameters\n",
        "    Returns:\n",
        "        path: A string with the hyperparameter name and value concatenated\n",
        "    \"\"\"\n",
        "    path = \"/content/gdrive/MyDrive/APS360/360Project/Models/model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
        "                                                   batch_size,\n",
        "                                                   learning_rate,\n",
        "                                                   epoch)\n",
        "    return path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEX9DBDdbDWZ"
      },
      "outputs": [],
      "source": [
        "use_cuda = True\n",
        "model = ANNClassifier_GOOGLE()\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "  Google.cuda()\n",
        "  print(torch.cuda.get_device_name(0))\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "else:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "  \n",
        "#proper model\n",
        "train(model, [],lr=0.001, batch_size=batch_size, num_epochs=30, train_loader=train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYuYWB1EDbPc"
      },
      "outputs": [],
      "source": [
        "model_path = get_model_name(\"google_drop_out_full\", batch_size=batch_size, learning_rate=0.001, epoch=1)\n",
        "torch.save(model.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZRROl9D2VF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YB7TEHa2yYA"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=50,num_workers=0, shuffle=True)\n",
        "\n",
        "def get_test_accuracy(model):\n",
        "    data_loader = test_loader\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predict=[]\n",
        "    truth=[]\n",
        "    for imgs, labels in data_loader:\n",
        "        \n",
        "         \n",
        "        #############################################\n",
        "        #To Enable GPU Usage\n",
        "        #if torch.cuda.is_available():\n",
        "          #imgs = imgs.cuda()\n",
        "          #labels = labels.cuda()\n",
        "        #############################################\n",
        "        #改这里 change here to switch model\n",
        "        output = model(Google(imgs))\n",
        "        \n",
        "        #select index with maximum prediction score\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "        predict.append(pred)\n",
        "        truth.append(labels)\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += imgs.shape[0]\n",
        "    #return predict,truth\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ANNClassifier_GOOGLE()\n",
        "model_path = get_model_name(\"google_drop_out_full\", batch_size=400, learning_rate=0.001, epoch=10)\n",
        "model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "boI9q74DzOZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhVwIhWl4gC9"
      },
      "outputs": [],
      "source": [
        "#model.to('cpu')\n",
        "model.eval()\n",
        "get_test_accuracy(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "def draw_confussion_matrix(model,data_loader):\n",
        "  model.eval()\n",
        "  predict=[]\n",
        "  truth=[]\n",
        "  real_predict=[]\n",
        "  real_symbols=[]\n",
        "  for imgs, labels in data_loader:\n",
        "    output = model(Google(imgs))\n",
        "    pred = output.max(1, keepdim=True)[1]\n",
        "    predict.append(pred)\n",
        "    truth.append(labels)\n",
        "  for predict_batch in predict:\n",
        "    for predicts in predict_batch:\n",
        "      real_predict.append(classes[predicts.item()])\n",
        "  for symbol_batch in truth:\n",
        "    for symbol in symbol_batch:\n",
        "      real_symbols.append(classes[symbol.item()])\n",
        "  print(len(real_predict),len(real_symbols))\n",
        "  df_cm = pd.DataFrame(confusion_matrix(real_symbols,real_predict), index = [i for i in classes],\n",
        "                  columns = [i for i in classes])\n",
        "  plt.figure(figsize = (10,7))\n",
        "  sn.heatmap(df_cm, annot=True) #cite:https://seaborn.pydata.org/generated/seaborn.heatmap.html\n"
      ],
      "metadata": {
        "id": "5PWxSvE-Xor1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=50,num_workers=0, shuffle=True)\n",
        "draw_confussion_matrix(model,test_loader)"
      ],
      "metadata": {
        "id": "HtIVOM-OYnYq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
